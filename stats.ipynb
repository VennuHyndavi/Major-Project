{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from nltk import FreqDist\n",
    "import pickle\n",
    "import sys\n",
    "from utils import write_status\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Takes in a preprocessed CSV file and gives statistics\n",
    "# Writes the frequency distribution of words and bigrams\n",
    "# to pickle files.\n",
    "\n",
    "\n",
    "def analyze_tweet(tweet):\n",
    "    result = {}\n",
    "    result['MENTIONS'] = tweet.count('USER_MENTION')\n",
    "    result['URLS'] = tweet.count('URL')\n",
    "    result['POS_EMOS'] = tweet.count('EMO_POS')\n",
    "    result['NEG_EMOS'] = tweet.count('EMO_NEG')\n",
    "    tweet = tweet.replace('USER_MENTION', '').replace(\n",
    "        'URL', '')\n",
    "    words = tweet.split()\n",
    "    result['WORDS'] = len(words)\n",
    "    bigrams = get_bigrams(words)\n",
    "    result['BIGRAMS'] = len(bigrams)\n",
    "    return result, words, bigrams\n",
    "\n",
    "\n",
    "def get_bigrams(tweet_words):\n",
    "    bigrams = []\n",
    "    num_words = len(tweet_words)\n",
    "    for i in range(num_words - 1):\n",
    "        bigrams.append((tweet_words[i], tweet_words[i + 1]))\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "def get_bigram_freqdist(bigrams):\n",
    "    freq_dict = {}\n",
    "    for bigram in bigrams:\n",
    "        if freq_dict.get(bigram):\n",
    "            freq_dict[bigram] += 1\n",
    "        else:\n",
    "            freq_dict[bigram] = 1\n",
    "    counter = Counter(freq_dict)\n",
    "    return counter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: python stats.py <preprocessed-CSV>\n",
      "Processing 300000/300000\n",
      "Calculating frequency distribution\n",
      "Saved uni-frequency distribution to C:\\Users\\Vyshnavi\\Desktop\\main\\dataset\\test-processed-freqdist.pkl\n",
      "Saved bi-frequency distribution to C:\\Users\\Vyshnavi\\Desktop\\main\\dataset\\test-processed-freqdist-bi.pkl\n",
      "\n",
      "[Analysis Statistics]\n",
      "User Mentions => Total: 297470, Avg: 0.9916, Max: 12\n",
      "URLs => Total: 9486, Avg: 0.0316, Max: 4\n",
      "Emojis => Total: 3410, Positive: 3043, Negative: 367, Avg: 0.0114, Max: 5\n",
      "Words => Total: 3626166, Unique: 93458, Avg: 12.0872, Max: 40, Min: 0\n",
      "Bigrams => Total: 3327647, Unique: 879826, Avg: 11.0922\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    if len(sys.argv) != 2:\n",
    "        print ('Usage: python stats.py <preprocessed-CSV>')\n",
    "        exit()\n",
    "    num_tweets, num_pos_tweets, num_neg_tweets = 0, 0, 0\n",
    "    num_mentions, max_mentions = 0, 0\n",
    "    num_emojis, num_pos_emojis, num_neg_emojis, max_emojis = 0, 0, 0, 0\n",
    "    num_urls, max_urls = 0, 0\n",
    "    num_words, num_unique_words, min_words, max_words = 0, 0, 1e6, 0\n",
    "    num_bigrams, num_unique_bigrams = 0, 0\n",
    "    all_words = []\n",
    "    all_bigrams = []\n",
    "    with open(r\"C:\\Users\\Vyshnavi\\Desktop\\main\\dataset\\test-processed.csv\", 'r') as csv:\n",
    "        header = next(csv)\n",
    "        lines = csv.readlines()\n",
    "        num_tweets = len(lines)\n",
    "        #print(lines[2])\n",
    "        for i, line in enumerate(lines):\n",
    "            t_id, tweet = line.strip().split(',')\n",
    "            #if_pos = int(if_pos)\n",
    "\n",
    "            #if if_pos:\n",
    "             #   num_pos_tweets += 1\n",
    "            #else:\n",
    "              #  num_neg_tweets += 1\n",
    "            result, words, bigrams = analyze_tweet(tweet)\n",
    "            num_mentions += result['MENTIONS']\n",
    "            max_mentions = max(max_mentions, result['MENTIONS'])\n",
    "            num_pos_emojis += result['POS_EMOS']\n",
    "            num_neg_emojis += result['NEG_EMOS']\n",
    "            max_emojis = max(\n",
    "                max_emojis, result['POS_EMOS'] + result['NEG_EMOS'])\n",
    "            num_urls += result['URLS']\n",
    "            max_urls = max(max_urls, result['URLS'])\n",
    "            num_words += result['WORDS']\n",
    "            min_words = min(min_words, result['WORDS'])\n",
    "            max_words = max(max_words, result['WORDS'])\n",
    "            all_words.extend(words)\n",
    "            num_bigrams += result['BIGRAMS']\n",
    "            all_bigrams.extend(bigrams)\n",
    "            write_status(i + 1, num_tweets)\n",
    "    num_emojis = num_pos_emojis + num_neg_emojis\n",
    "    unique_words = list(set(all_words))\n",
    "    with open(r\"C:\\Users\\Vyshnavi\\Desktop\\main\\dataset\\test-processed.csv\"[:-4] + '-unique.txt', 'w') as uwf:\n",
    "        uwf.write('\\n'.join(unique_words))\n",
    "    num_unique_words = len(unique_words)\n",
    "    num_unique_bigrams = len(set(all_bigrams))\n",
    "    print ('\\nCalculating frequency distribution')\n",
    "    # Unigrams\n",
    "    freq_dist = FreqDist(all_words)\n",
    "    pkl_file_name = r\"C:\\Users\\Vyshnavi\\Desktop\\main\\dataset\\test-processed.csv\"[:-4] + '-freqdist.pkl'\n",
    "    with open(pkl_file_name, 'wb') as pkl_file:\n",
    "        pickle.dump(freq_dist, pkl_file)\n",
    "    print ('Saved uni-frequency distribution to %s' % pkl_file_name)\n",
    "    # Bigrams\n",
    "    bigram_freq_dist = get_bigram_freqdist(all_bigrams)\n",
    "    bi_pkl_file_name = r\"C:\\Users\\Vyshnavi\\Desktop\\main\\dataset\\test-processed.csv\"[:-4] + '-freqdist-bi.pkl'\n",
    "    with open(bi_pkl_file_name, 'wb') as pkl_file:\n",
    "        pickle.dump(bigram_freq_dist, pkl_file)\n",
    "    print ('Saved bi-frequency distribution to %s' % bi_pkl_file_name)\n",
    "    print ('\\n[Analysis Statistics]')\n",
    "    #print ('Tweets => Total: %d, Positive: %d, Negative: %d' % (num_tweets, num_pos_tweets, num_neg_tweets))\n",
    "    print ('User Mentions => Total: %d, Avg: %.4f, Max: %d' % (num_mentions, num_mentions / float(num_tweets), max_mentions))\n",
    "    print ('URLs => Total: %d, Avg: %.4f, Max: %d' % (num_urls, num_urls / float(num_tweets), max_urls))\n",
    "    print ('Emojis => Total: %d, Positive: %d, Negative: %d, Avg: %.4f, Max: %d' % (num_emojis, num_pos_emojis, num_neg_emojis, num_emojis / float(num_tweets), max_emojis))\n",
    "    print ('Words => Total: %d, Unique: %d, Avg: %.4f, Max: %d, Min: %d' % (num_words, num_unique_words, num_words / float(num_tweets), max_words, min_words))\n",
    "    print ('Bigrams => Total: %d, Unique: %d, Avg: %.4f' % (num_bigrams, num_unique_bigrams, num_bigrams / float(num_tweets)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
